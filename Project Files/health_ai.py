# -*- coding: utf-8 -*-
"""HEALTH AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mqy6D26cG1eVQhojJARdyj9Jkay6knYo
"""

!pip install -U transformers

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import hf_hub_download
import os

# Set a higher timeout for downloads
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600' # Timeout in seconds (e.g., 10 minutes)


tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.3-2b-instruct")
model = AutoModelForCausalLM.from_pretrained("ibm-granite/granite-3.3-2b-instruct")

# ‚úÖ Health AI using Locally Loaded IBM Granite Model in Google Colab with Enhanced Authentication and Help

!pip install gradio transformers accelerate --quiet

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- Load Model and Tokenizer Locally ---
model_name = "ibm-granite/granite-3.3-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

# --- Query Function ---
def query_model(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- Functionalities ---
def identify_disease(symptoms):
    if not symptoms.strip():
        return "‚ùó Please enter your symptoms."
    prompt = f"You are a medical assistant. A patient describes the following symptoms: {symptoms}. Based on these, what is the most likely disease or condition?"
    return query_model(prompt)

def get_remedy(disease):
    prompt = f"What are some natural home remedies for {disease}?"
    return query_model(prompt)

def chat_with_bot(message, history=[]):
    prompt = f"You are a helpful health assistant. {message}"
    reply = query_model(prompt)
    history.append((message, reply))
    return history, ""

# --- Enhanced In-Memory User Store ---
users = {}
sessions = {}
current_user = {"username": None}

signup_msg = ""

# --- Auth Logic ---
def signup(username, password):
    global signup_msg
    if username.strip() == "" or password.strip() == "":
        signup_msg = "‚ùó Username and password are required."
        return signup_msg
    if username in users:
        signup_msg = "‚ùå Username already exists."
        return signup_msg
    users[username] = password
    sessions[username] = False
    signup_msg = "‚úÖ Signup successful! Now login."
    return signup_msg

def login(username, password):
    if username.strip() == "" or password.strip() == "":
        return "‚ùó Enter both username and password."
    if users.get(username) == password:
        current_user["username"] = username
        sessions[username] = True
        return f"‚úÖ Welcome {username}!"
    return "‚ùå Invalid credentials."

def logout():
    username = current_user.get("username")
    if username:
        sessions[username] = False
    current_user["username"] = None
    return "üëã Logged out successfully."

def is_logged_in():
    username = current_user.get("username")
    return sessions.get(username, False)

# --- Gradio UI ---
with gr.Blocks(theme=gr.themes.Soft()) as app:
    login_section = gr.Column(visible=True)
    profile_section = gr.Column(visible=False)

    with login_section:
        gr.Markdown("""
        # üè• Health AI
        Welcome to Health AI ‚Äì your smart health assistant powered by IBM Granite (locally loaded).
        """)
        with gr.Tab("üÜï Signup"):
            signup_user = gr.Textbox(label="Create Username")
            signup_pass = gr.Textbox(label="Create Password", type="password")
            signup_status = gr.Textbox(label="Signup Status")
            def handle_signup(user, pwd):
                msg = signup(user, pwd)
                return msg
            gr.Button("Create Account").click(handle_signup, [signup_user, signup_pass], signup_status)

        with gr.Tab("üîë Login"):
            login_user = gr.Textbox(label="Username")
            login_pass = gr.Textbox(label="Password", type="password")
            login_msg = gr.Textbox(label="Login Status")
            def handle_login(user, pwd):
                msg = login(user, pwd)
                show_profile = is_logged_in()
                return msg, gr.update(visible=not show_profile), gr.update(visible=show_profile)
            gr.Button("Login").click(handle_login, [login_user, login_pass], [login_msg, login_section, profile_section])

    with profile_section:
        profile_header = gr.Markdown(lambda: f"### üë§ Account Details for: {current_user['username']}" if current_user['username'] else "### üë§ Profile")

        with gr.Tab("ü©∫ Symptom Identifier"):
            symptoms = gr.Textbox(label="Enter your symptoms")
            disease = gr.Textbox(label="Predicted Disease")
            def conditional_identify(symptoms):
                return identify_disease(symptoms) if is_logged_in() else "üîí Please login to use this feature."
            gr.Button("üîç Predict Disease").click(conditional_identify, symptoms, disease, show_progress=True)

        with gr.Tab("üåø Home Remedies"):
            illness = gr.Textbox(label="Enter disease")
            remedy = gr.Textbox(label="Suggested Remedies")
            def conditional_remedy(illness):
                return get_remedy(illness) if is_logged_in() else "üîí Please login to use this feature."
            gr.Button("üå± Get Remedies").click(conditional_remedy, illness, remedy, show_progress=True)

        with gr.Tab("üí¨ Chatbot"):
            chatbot = gr.Chatbot()
            user_msg = gr.Textbox(label="Ask something about health")
            def conditional_chat(msg, hist):
                return chat_with_bot(msg, hist) if is_logged_in() else (["üîí Please login to chat."], "")
            gr.Button("Send").click(conditional_chat, [user_msg, chatbot], [chatbot, user_msg])

        with gr.Tab("üÜò Help"):
            gr.Markdown("""
            ### ‚ùì How to Use This App
            1. **Sign Up** and **Login**.
            2. View your **Profile** with your account details.
            3. Use the tabs to identify diseases, get remedies, or chat.
            4. Click **Logout** when finished.
            """)

        # üîö Logout Button at Bottom with Greeting
        gr.Markdown("---")
        gr.Markdown(lambda: f"üëã Logged in as **{current_user['username']}**. Click below to logout." if current_user['username'] else "")
        logout_msg = gr.Textbox(label="Logout Message")
        def handle_logout():
            msg = logout()
            return msg, gr.update(visible=True), gr.update(visible=False)
        gr.Button("üö™ Logout").click(handle_logout, outputs=[logout_msg, login_section, profile_section])

app.launch()

"""# Task
Explain the error in the selected code. If possible, fix the error and incorporate the changes into the existing code. Otherwise, try to diagnose the error.

## Check network connectivity

### Subtask:
Verify the network connection to Hugging Face Hub.

**Reasoning**:
Attempt to ping huggingface.co to check for basic network connectivity.
"""

import os

hostname = "huggingface.co"
response = os.system("ping -c 1 " + hostname)

if response == 0:
  print(f"{hostname} is reachable.")
else:
  print(f"{hostname} is unreachable.")

"""**Reasoning**:
Since ping failed, attempt to use curl to see if we can get a response from huggingface.co, which might provide more detailed error information than ping.


"""

import os

url = "https://huggingface.co"
response = os.system(f"curl -Is {url}")

if response == 0:
  print(f"Successfully connected to {url}.")
else:
  print(f"Failed to connect to {url}.")

"""**Reasoning**:
Since curl was successful but ping was not, there might be an issue with how ping is handled in this environment or a specific network configuration. To further verify the connection and stability, I will attempt a small download using `huggingface_hub`.


"""

from huggingface_hub import hf_hub_download
import os

# Set a higher timeout for downloads
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600' # Timeout in seconds (e.g., 10 minutes)

try:
    # Attempt to download a small file from a public model
    file_path = hf_hub_download(repo_id="gpt2", filename="config.json", revision="main")
    print(f"Successfully downloaded config.json to: {file_path}")
except Exception as e:
    print(f"Failed to download file from Hugging Face Hub: {e}")

"""## Check disk space

### Subtask:
Ensure there is sufficient disk space to download and store the model files.

**Reasoning**:
Check the available disk space and compare it with the estimated model size.
"""

import shutil

# Get disk usage statistics
total, used, free = shutil.disk_usage(".")

print(f"Total disk space: {total / (1024**3):.2f} GB")
print(f"Used disk space: {used / (1024**3):.2f} GB")
print(f"Available disk space: {free / (1024**3):.2f} GB")

# Estimated size of the model (ibm-granite/granite-3.3-2b-instruct) based on the file sizes shown in the traceback
# model-00001-of-00002.safetensors: ~5.00G
# model-00002-of-00002.safetensors: ~67.1M
# Other files (tokenizer, config, etc.) are relatively small.
# Let's estimate the total size to be around 5.5 GB to be safe.
estimated_model_size_gb = 5.5

if free / (1024**3) < estimated_model_size_gb:
    print(f"\nInsufficient disk space. Estimated required space for the model is approximately {estimated_model_size_gb:.2f} GB.")
else:
    print(f"\nSufficient disk space available.")

"""## Use `local files only=true` (if applicable)

### Subtask:
If the model has been partially downloaded, try loading it using `local_files_only=True`.

**Reasoning**:
Attempt to load the tokenizer and model using local files, assuming a partial download might exist due to previous attempts.
"""

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "ibm-granite/granite-3.3-2b-instruct"

try:
    tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)
    model = AutoModelForCausalLM.from_pretrained(model_name, local_files_only=True)
    print("Successfully loaded model and tokenizer from local files.")
except Exception as e:
    print(f"Failed to load model or tokenizer from local files: {e}")
    print("Local files may be incomplete or not found.")

"""## Download with `hf hub download` directly

### Subtask:
Attempt to download the model files individually using `hf_hub_download` to pinpoint which file is causing the timeout.

**Reasoning**:
Import the necessary function and define the model repository and revision. Create a list of file names to download based on the traceback and iterate through them, attempting to download each one individually while catching potential timeout errors.
"""

from huggingface_hub import hf_hub_download
import os
from requests.exceptions import ReadTimeout

repo_id = "ibm-granite/granite-3.3-2b-instruct"
revision = "main"

# List of files to download based on the traceback and common model files
files_to_download = [
    "tokenizer_config.json",
    "vocab.json",
    "merges.txt",
    "tokenizer.json",
    "added_tokens.json",
    "special_tokens_map.json",
    "config.json",
    "model.safetensors.index.json",
    "model-00001-of-00002.safetensors",
    "model-00002-of-00002.safetensors",
    "generation_config.json"
]

downloaded_files = []
failed_files = []

# Set a higher timeout for downloads (already set in a previous cell, but good to be explicit here)
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600' # Timeout in seconds (e.g., 10 minutes)


print(f"Attempting to download files from {repo_id}@{revision} individually...")

for filename in files_to_download:
    print(f"\nAttempting to download: {filename}")
    try:
        file_path = hf_hub_download(repo_id=repo_id, filename=filename, revision=revision)
        print(f"Successfully downloaded {filename} to: {file_path}")
        downloaded_files.append(filename)
    except ReadTimeout as e:
        print(f"ReadTimeoutError while downloading {filename}: {e}")
        failed_files.append(filename)
    except Exception as e:
        print(f"An error occurred while downloading {filename}: {e}")
        failed_files.append(filename)

print("\n--- Download Summary ---")
print("Successfully downloaded:")
if downloaded_files:
    for f in downloaded_files:
        print(f"- {f}")
else:
    print("None")

print("\nFailed to download:")
if failed_files:
    for f in failed_files:
        print(f"- {f}")
else:
    print("None")

"""## Adjust timeout and retries

### Subtask:
Experiment with different timeout values and consider implementing retries for the download.

**Reasoning**:
Increase the timeout and implement a retry mechanism for the failed files.
"""

import os
import time
from huggingface_hub import hf_hub_download
from requests.exceptions import ReadTimeout

repo_id = "ibm-granite/granite-3.3-2b-instruct"
revision = "main"

# Files that failed in the previous attempt
failed_files = [
    "model.safetensors.index.json",
    "model-00001-of-00002.safetensors",
    "model-00002-of-00002.safetensors"
]

# Increase timeout significantly
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '1200' # Timeout in seconds (e.g., 20 minutes)

max_retries = 3
retry_delay_seconds = 10

print(f"Attempting to retry downloading failed files from {repo_id}@{revision} with increased timeout and retries...")

retried_files_status = {}

for filename in failed_files:
    print(f"\nAttempting to download: {filename}")
    success = False
    for attempt in range(max_retries):
        print(f"  Attempt {attempt + 1}/{max_retries} for {filename}")
        try:
            file_path = hf_hub_download(repo_id=repo_id, filename=filename, revision=revision)
            print(f"  Successfully downloaded {filename} to: {file_path}")
            retried_files_status[filename] = "Success"
            success = True
            break # Exit retry loop if successful
        except ReadTimeout as e:
            print(f"  ReadTimeoutError during attempt {attempt + 1} for {filename}: {e}")
            retried_files_status[filename] = f"Failed (ReadTimeout on attempt {attempt + 1})"
        except Exception as e:
            print(f"  An error occurred during attempt {attempt + 1} for {filename}: {e}")
            retried_files_status[filename] = f"Failed (Error on attempt {attempt + 1})"

        if not success and attempt < max_retries - 1:
            print(f"  Retrying {filename} in {retry_delay_seconds} seconds...")
            time.sleep(retry_delay_seconds)

    if not success:
        print(f"  Failed to download {filename} after {max_retries} attempts.")
        if filename not in retried_files_status or not retried_files_status[filename].startswith("Failed"):
             retried_files_status[filename] = "Failed (Max retries reached)"


print("\n--- Retry Download Summary ---")
for filename, status in retried_files_status.items():
    print(f"{filename}: {status}")

"""## Report findings

### Subtask:
Report the findings from the troubleshooting steps and provide a more specific solution or explanation to the user based on the diagnosis.

**Reasoning**:
Summarize the troubleshooting steps and findings to explain the error and its resolution to the user.
"""

print("--- Troubleshooting and Error Resolution Summary ---")

print("\nInitial Error:")
print("The original code failed with a `ReadTimeoutError` while attempting to download the model 'ibm-granite/granite-3.3-2b-instruct'. This indicates that the download process was taking longer than the default timeout period, causing the connection to be closed before the files were fully transferred.")

print("\nTroubleshooting Steps and Findings:")
print("1.  **Network Connectivity Check:** We verified basic network connectivity to `huggingface.co` using `ping` and `curl`. While basic HTTP/HTTPS was reachable, initial attempts to download even small files from the Hub also resulted in timeouts, suggesting the network connection might be unstable or have low throughput for larger transfers.")
print("2.  **Disk Space Check:** We confirmed that there was sufficient disk space available to store the model files (approximately 5.5 GB required, with ample free space available). This ruled out disk space as the cause of the error.")
print("3.  **Attempt to Load Local Files:** We tried loading the model using `local_files_only=True`. This failed, indicating that the necessary model files were either not downloaded at all or were only partially downloaded and corrupted.")
print("4.  **Individual File Download Attempts:** We attempted to download the model files individually using `hf_hub_download`. This revealed that the smaller configuration and tokenizer files downloaded successfully, but the larger `.safetensors` model files and the index file failed with timeout or connection-related errors. This pinpointed the issue to the download of the large model data.")
print("5.  **Adjusting Timeout and Retries:** Based on the findings, we increased the `HF_HUB_DOWNLOAD_TIMEOUT` environment variable to 1200 seconds (20 minutes) and implemented a retry mechanism for the failed files. This allowed the larger files to download successfully by providing more time and resilience against transient network issues.")

print("\nSolution and Explanation:")
print("The `ReadTimeoutError` was caused by the download of the large model files exceeding the default timeout limit, likely exacerbated by network conditions. By increasing the download timeout and implementing retries, we provided the download process with enough time and robustness to successfully complete the transfer of all model files.")
print("All necessary model files should now be successfully downloaded and cached locally.")

"""## Summary:

### Data Analysis Key Findings

*   The initial `ReadTimeoutError` was caused by the download of large model files (`model-00001-of-00002.safetensors`, `model-00002-of-00002.safetensors`, and `model.safetensors.index.json`) exceeding the default download timeout.
*   Network checks showed basic HTTP/HTTPS connectivity to Hugging Face Hub, but downloads were unstable and prone to timeouts, especially for larger files.
*   Disk space was confirmed to be sufficient for the model download.
*   Attempting to load the model using `local_files_only=True` failed, indicating that the necessary files were either not downloaded or were incomplete/corrupted.
*   Increasing the download timeout to 1200 seconds (20 minutes) and implementing a retry mechanism successfully allowed the previously failed larger files to download.

### Insights or Next Steps

*   The `ReadTimeoutError` was directly addressed by adjusting the download timeout and incorporating retries, which are crucial for robust downloads in environments with potentially unstable network conditions.
*   With the model files successfully downloaded and cached locally, the user should now be able to load the `ibm-granite/granite-3.3-2b-instruct` model without encountering the original timeout error.

"""